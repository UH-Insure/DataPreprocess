#!/usr/bin/env python3
"""
make_alpaca_instructions.py

Generate Alpaca-format rows (instruction/input/output) from source code files
by asking the OpenAI API to write a *spec-writing* instruction for each file.

Output: JSONL with fields:
  - instruction : str   (generated by the model; spec-writing style)
  - input       : str   (empty or a trimmed excerpt of the code)
  - output      : str   (left empty "" so you can fill later)

Optionally, generate a Batch API requests.jsonl for large-scale offline runs.

Usage:
  python make_alpaca_instructions.py \
      --src ./my_sources \
      --out alpaca_instructions.jsonl \
      --model gpt-4.1-mini \
      --include-ext .cry .saw .c .h \
      --input-mode excerpt \
      --max-chars 6000 \
      --batch-file requests.jsonl   # optional: create Batch API jobs file
"""
import re
import tiktoken
import argparse
import json
import os
import dotenv
import sys
import time
from pathlib import Path
from typing import Iterable, List, Dict, Any, Tuple, Optional
import pandas as pd
from pydantic import BaseModel
from src.util.file_kv_cache import FileKVCache
from . import sft_cryptol
from . import sft_saw
dotenv.load_dotenv()
# -------- Configurable defaults --------
DEFAULT_EXTS = [".cry", ".saw", ".c", ".h"]
DEFAULT_MODEL = "gpt-4.1-mini"
MAX_RETRIES = 2
BACKOFF_BASE = 1.8
CRYPTOL_VECTOR_STORE_ID = dotenv.get_key("CRYPTOL_VECTOR_STORE_ID", "")
# Heuristic: keep code snippet small so prompt stays within token budget.
DEFAULT_MAX_CHARS = 6000

# ---- System prompt tuned for *spec-writing* instructions ----
SYSTEM_PROMPT = (
    "You write *spec-writing instructions* for software verification or specification tasks.\n"
    "Given a code snippet, return exactly one instruction suitable for the Alpaca dataset.\n"
    "The instruction should ask a model to *write specifications* or *verification script* or *write an implementation*, e.g.,\n"
    "  - define a property that verifies the behavior of the values within the code snippet.\n"
    "  - write an implementation of the algorithm in Cryptol,\n"
    "  - write a SAWScript llvm_verify that verifies the implementation against the Cryptol specification,\n"
    "  - prove equivalence between two implementations, etc.\n"
    "The instruction must be standalone, concise (<= 150 words), and contain no solution.\n"
)

SYSTEM_PROMPT_CRYPTOL = (
    "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n"
)

SYSTEM_PROMPT_SAW = (
    "Return exactly ONE fenced code block labeled `saw` and nothing else (no prose before/after).\n"
)

# ---- User template: supplies code and file metadata ----
USER_TEMPLATE = (
    "File path: {filename}\n"
    "Language hint: {lang}\n"
    "Goal: Produce one *instruction* asking the assistant to write a specification or verification artifact for this file.\n"
    "Return ONLY a JSON object with keys: instruction, input, output. Set 'output' to an empty string.\n"
    "If you include an 'input', keep it very short or empty.\n\n"
    "Code excerpt:\n"
    "-----8<-----\n{code}\n-----8<-----\n"
)

class AlpacaRow(BaseModel):
    instruction: str
    input: str
    output: str

class QAPair(BaseModel):
    question: str
    answer: str


class QAPairList(BaseModel):
    """
    Container for multiple Q&A pairs derived from a single scraped web page.
    """
    qa_pairs: List[QAPair]


SYSTEM_PROMPT_QA = (
    "You are an expert tutor. Given a single scraped web page in Markdown, you "
    "write multiple high-quality question–answer pairs that cover the page's "
    "important facts and concepts.\n"
    "Questions must be self-contained and understandable without seeing the "
    "page; answers should be concise but informative.\n"
    "Avoid meta-questions about the document itself; focus on its content."
)


# ---- Response schema: forces Alpaca fields ----
ALPACA_SCHEMA = {
    "name": "AlpacaRow",
    "strict": True,
    "schema": {
        "type": "object",
        "required": ["instruction", "input", "output"],
        "properties": {
            "instruction": {"type": "string"},
            "input": {"type": "string"},
            "output": {"type": "string"},
        },
        "additionalProperties": False,
    },
}

TOKEN_LIMITS = {
    "gpt-5.1": 30000,
}
# Light language mapping for context
LANG_HINT = {
    ".cry": "Cryptol",
    ".saw": "SAWScript",
    ".c": "C",
    ".h": "C",
    ".py": "Python",
    ".rs": "Rust",
    ".cpp": "C++",
    ".hpp": "C++",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
}

# Recommended: keep this focused on your spec/verification tasks
DEFAULT_SYSTEM = (
    "You are a meticulous assistant that writes formal specifications and "
    "verification harnesses (Cryptol properties, SAW llvm_verify scripts, "
    "equivalence proofs). Follow the user's instruction exactly; do not include "
    "explanations unless asked."
)
enc = tiktoken.get_encoding("o200k_base")
def count_tokens_for_messages(messages):
    # Simple approximate scheme: concatenate role + content.
    # For exact ChatML accounting there is some small overhead, but this is
    # good enough for routing.
    text_chunks = []
    for m in messages:
        text_chunks.append(m["role"])
        text_chunks.append(m["content"])
    full_text = "\n".join(text_chunks)

    tokens = enc.encode(full_text)
    return len(tokens)

def _build_user_content(instruction: str, alpaca_input: str, filename: Optional[str]) -> str:
    parts = [f"{instruction.strip()}"]
    #if filename:
    #    parts.append(f"### File:\n{filename}")
    if alpaca_input and alpaca_input.strip():
        parts.append(f"`{alpaca_input.strip()}`")
    return "\n".join(parts)

def explode_qa_pairs_overwrite_content(df: pd.DataFrame) -> pd.DataFrame:
    """
    From a df where each row has:
      - 'content'  : full Markdown page
      - 'qa_pairs' : list of {'question', 'answer'}

    Produce a new df where each row is ONE Q&A and:
      - instruction = question
      - content     = answer          (overwrites original content)
      - input       = ""              (or customize)
      - output      = ""              (left empty as requested)
      - original page is kept in 'page_content'
    """
    # Only rows with QA pairs
    df_qa = df[df["qa_pairs"].notna()].copy()

    # Explode to one row per QA pair
    df_qa = df_qa.explode("qa_pairs", ignore_index=True)

    # Preserve original page, then overwrite content with the answer
    df_qa["page_content"] = df_qa["content"]
    df_qa["instruction"] = df_qa["qa_pairs"].apply(lambda qa: qa["question"])
    df_qa["content"] = df_qa["qa_pairs"].apply(lambda qa: qa["answer"])

    # Alpaca-style fields
    df_qa["input"] = ""   # or a snippet of page_content if you want context
    df_qa["output"] = ""  # explicitly empty

    # Make sure filetype is something sensible
    df_qa["filetype"] = df_qa["filetype"].fillna("text")

    # Drop the list-of-QAs column if not needed
    df_qa = df_qa.drop(columns=["qa_pairs"])

    return df_qa
def alpaca_df_to_qwen_messages(
    df: pd.DataFrame,
    output: str,
    #system_prompt: str = DEFAULT_SYSTEM,
    drop_input: bool = True,
    include_filename_in_user: bool = True,
) -> pd.DataFrame:
    """
    Convert a DataFrame with columns ['instruction','input', <output>, 'filename'?]
    into rows of {'messages': [{role,content}...]} for Qwen chat SFT.

    Behavior by filetype:
      - 'cryptol' : system = SYSTEM_PROMPT_CRYPTOL, assistant = ```cryptol ... ```
      - 'saw'     : system = SYSTEM_PROMPT_SAW,     assistant = ```saw ... ```
      - 'text'    : system = SYSTEM_PROMPT_QA,      assistant = plain answer text
      - other     : no special system prompt, assistant = plain answer text
    """
    records = []
    for _, r in df.iterrows():
        filetype = r.get("filetype", "") or ""
        answer_text = (r.get(output) or "").strip()

        # ----- Assistant message content -----
        if filetype == "cryptol":
            out = f"```cryptol\n{answer_text}\n```"
        elif filetype == "saw":
            out = f"```saw\n{answer_text}\n```"
        elif filetype == "text":
            # Q&A: plain natural-language answer (no code fences)
            out = answer_text
        else:
            # Fallback: plain text as well
            out = answer_text

        # ----- User message content (question + optional input) -----
        user = _build_user_content(
            instruction=r["instruction"],
            alpaca_input=None if drop_input else r.get("input", ""),
            filename=r["filename"] if (include_filename_in_user and "filename" in r) else None,
        )

        # ----- System prompt selection -----
        if filetype == "cryptol":
            system_content = SYSTEM_PROMPT_CRYPTOL
        elif filetype == "saw":
            system_content = SYSTEM_PROMPT_SAW
        elif filetype == "text":
            system_content = ""
        else:
            system_content = ""

        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user},
            {"role": "assistant", "content": out},
        ]

        rec = {"messages": messages}
        # keep useful metadata alongside
        for k in ("filename", "filetype", "set"):
            if k in r:
                rec[k] = r[k]
        records.append(rec)

    return pd.DataFrame(records)

def write_jsonl(rows: Iterable[Dict[str, Any]], path: str) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

# ----- CLI helpers (optional) -----
def from_alpaca_jsonl_to_qwen_messages(in_path: str, out_path: str, **kwargs) -> None:
    """
    Read an Alpaca JSONL with fields {instruction,input,output,(filename?),(filetype?)}
    and write Qwen chat messages JSONL.
    """
    df = pd.read_json(in_path, orient="records", lines=True)
    chat_df = alpaca_df_to_qwen_messages(df, **kwargs)
    write_jsonl(chat_df.to_dict(orient="records"), out_path)

def build_user_prompt(filename: str, lang: str, code: str, input_mode: str) -> Tuple[str, str]:
    if input_mode == "none":
        alpaca_input = ""
        code_for_prompt = code
    elif input_mode == "excerpt":
        alpaca_input = code[:512]
        code_for_prompt = code
    else:  
        alpaca_input = code
        code_for_prompt = code

    user = USER_TEMPLATE.format(filename=filename, lang=lang, code=code_for_prompt)
    return user, alpaca_input

def call_openai_structured(
        model: str, 
        system: str, 
        user: str,
        source_code: str = "",
        text_format: type[BaseModel] = AlpacaRow,
        ) -> Dict[str, Any]:
    """
    Calls the OpenAI Responses API using a Pydantic 'text_format'
    to force a particular JSON shape.

    For code-spec generation we use AlpacaRow; for text Q&A generation
    we use QAPairList.
    """
    from openai import OpenAI, RateLimitError
    client = OpenAI()
    tools = [{
        "type": "file_search",
        "vector_store_ids": ["vs_691cd78f3e088191a660732e83652938"],
        "max_num_results": 3,
    }]
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            tokens = count_tokens_for_messages(
                [
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
            )
            response = client.responses.parse(
                model= "gpt-5-2025-08-07" if model in TOKEN_LIMITS.keys() and tokens > TOKEN_LIMITS[model] else model,
                input=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
                text_format=text_format,
                tools=tools,
            )
            print(f"Response:\n\n{response}")
            parsed = response.output_parsed.model_dump()
            # For code, you optionally append source_code into the instruction.
            if source_code and isinstance(parsed, dict) and "instruction" in parsed:
                parsed["instruction"] += f"\n\n# Source code:\n{source_code}"
            return parsed
        except RateLimitError as e:
            # (unchanged retry logic...)
            wait_seconds = None
            try:
                retry_after = e.response.headers.get("retry-after") if e.response else None
                if retry_after is not None:
                    wait_seconds = float(retry_after)
            except Exception:
                pass
            if wait_seconds is None:
                m = re.search(r"Please try again in ([0-9.]+)s", str(e))
                if m:
                    time.sleep(1)
                    wait_seconds = float(m.group(1))
            if wait_seconds is None:
                wait_seconds = min(60, 2 ** attempt)

            print(f"Rate limit hit: {e}")
            print(f"Sleeping for {wait_seconds:.2f} seconds before retry #{attempt+1}...")
            time.sleep(wait_seconds)
        except Exception as e:
            last_err = e
            if attempt == MAX_RETRIES:
                raise
            sleep_s = BACKOFF_BASE ** (attempt - 1)
            time.sleep(sleep_s)
    raise last_err  # should not get here

def build_prompt_call_openai_structured(
    model: str,
    input_mode: str,
    filename: str,
    lang: str,
    code: str,
) -> Dict[str, Any]:
    system_prompt = ""
    if lang == "cryptol":
        # Existing Cryptol branch
        user, _ = build_user_prompt(filename, lang, code, input_mode=input_mode)
        system_prompt = sft_cryptol.SYSTEM_PROMPT_CRYPTOL
        result = call_openai_structured(
            model,
            system_prompt,
            user,
            text_format=AlpacaRow,
        )
        return result

    elif lang == "text":
        # NEW: scraped Markdown web page -> multiple Q&A pairs
        user = (
            "You are given the full content of ONE scraped web page in Markdown format.\n"
            "Read the entire document and generate multiple high-quality question–answer "
            "pairs that could be used to quiz a student on this page.\n"
            "Each question must be answerable using ONLY the information in this page.\n"
            "Include both factual 'what/when/where' questions and deeper 'why/how' "
            "questions when appropriate.\n\n"
            f"Filename: {filename}\n\n"
            "-----8<----- BEGIN PAGE (Markdown) -----8<-----\n"
            f"{code}\n"
            "-----8<----- END PAGE (Markdown) -----8<-----\n"
        )
        system_prompt = SYSTEM_PROMPT_QA
        # Return Q&A array: { "qa_pairs": [ { "question": ..., "answer": ... }, ... ] }
        result = call_openai_structured(
            model,
            system_prompt,
            user,
            text_format=QAPairList,
        )
        return result

    else:
        # Existing SAW (or other) branch
        user, source_code, _ = sft_saw.build_user_prompt(filename, code)
        system_prompt = sft_saw.SYSTEM_PROMPT
        result = call_openai_structured(
            model,
            system_prompt,
            user,
            source_code=source_code,
            text_format=AlpacaRow,
        )
        return result

def iter_call_openai_structured(
    input_df: pd.DataFrame,
    model: str,
    input_mode: str,
    file_cache_path: str
) -> Iterable[Dict[str, Any]]:
    fileKVCache = FileKVCache(file_cache_path)
    returned_rows = []
    for _, row in input_df.iterrows():
        result = fileKVCache.get_or_call(
            row['filename'],
            build_prompt_call_openai_structured,
            {
                "model": model,
                "input_mode": input_mode,
                "filename": row['filename'],
                "lang": row['filetype'],
                "code": row['content'],
            }
        )
        returned_rows.append({
            "filename": row['filename'],
            "filetype": row['filetype'],
            "set": row['set'],
            **result,
            "content": row['content'],
        })
    return pd.DataFrame(returned_rows)

if __name__ == "__main__":
    pass