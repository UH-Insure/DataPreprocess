#!/usr/bin/env python3
"""
make_alpaca_instructions.py

Generate Alpaca-format rows (instruction/input/output) from source code files
by asking the OpenAI API to write a *spec-writing* instruction for each file.

Output: JSONL with fields:
  - instruction : str   (generated by the model; spec-writing style)
  - input       : str   (empty or a trimmed excerpt of the code)
  - output      : str   (left empty "" so you can fill later)

Optionally, generate a Batch API requests.jsonl for large-scale offline runs.

Usage:
  python make_alpaca_instructions.py \
      --src ./my_sources \
      --out alpaca_instructions.jsonl \
      --model gpt-4.1-mini \
      --include-ext .cry .saw .c .h \
      --input-mode excerpt \
      --max-chars 6000 \
      --batch-file requests.jsonl   # optional: create Batch API jobs file
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Iterable, List, Dict, Any, Tuple, Optional
import pandas as pd
from pydantic import BaseModel
from src.util.file_kv_cache import FileKVCache
from . import sft_cryptol
from . import sft_saw

# -------- Configurable defaults --------
DEFAULT_EXTS = [".cry", ".saw", ".c", ".h"]
DEFAULT_MODEL = "gpt-4.1-mini"
MAX_RETRIES = 2
BACKOFF_BASE = 1.8

# Heuristic: keep code snippet small so prompt stays within token budget.
DEFAULT_MAX_CHARS = 6000

# ---- System prompt tuned for *spec-writing* instructions ----
SYSTEM_PROMPT = (
    "You write *spec-writing instructions* for software verification or specification tasks.\n"
    "Given a code snippet, return exactly one instruction suitable for the Alpaca dataset.\n"
    "The instruction should ask a model to *write specifications* or *verification harnesses*, e.g.,\n"
    "  - write Cryptol properties suitable for :prove / :check,\n"
    "  - write a SAWScript llvm_verify harness for a function,\n"
    "  - prove equivalence between two implementations, etc.\n"
    "The instruction must be standalone, concise (<= 150 words), and contain no solution.\n"
)

SYSTEM_PROMPT_CRYPTOL = (
    "You write Cryptol code ONLY (types, functions, and properties).\n"
    "\n"
    "Output requirements:\n"
    "- Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n"
    "- No comments, no Markdown inside the code, ASCII only.\n"
    "- The code must load and parse in Cryptol (no REPL directives like :prove/:check).\n"
    "\n"
    "Behavioral rules:\n"
    "- Write succinct properties and supporting definitions that match the user’s request; avoid full module dumps unless asked.\n"
    "- Provide explicit type signatures; add necessary constraints (`=>`) to ensure totality and finiteness.\n"
    "- Use exact names and shapes from any associated source; prefer small, composable helpers over large rewrites.\n"
)

SYSTEM_PROMPT_SAW = (
    "You write SAWScript verification harnesses ONLY.\n"
    "\n"
    "Output requirements:\n"
    "- Return exactly ONE fenced code block labeled `saw` and nothing else (no prose before/after).\n"
    "- No comments, no Markdown inside the code, ASCII only.\n"
    "- The code must load and parse in SAW.\n"
    "\n"
    "Behavioral rules:\n"
    "- Infer backend from the user context: use `jvm_*` if Java/`java_load_class` is present; use `llvm_*` if LLVM/bitcode is referenced. Do NOT use `mir_*` unless explicitly asked.\n"
    "- Define a let-bound spec (e.g., `let <target>_spec = do { ...; };`)—do NOT pass an inline `do { ... }` as an argument.\n"
    "- Inside `do { ... }`, separate commands with semicolons.\n"
    "- Create symbolic inputs with `*_fresh_var`; add only minimal `*_precond` needed for safety/termination.\n"
    "- Invoke the exact target function/method with `*_execute_func [...]` using precise names/signatures from the associated source.\n"
    "- Specify the postcondition with `*_return (...)` (e.g., equalities over results/arrays/structs).\n"
    "- Load artifacts using `java_load_class` or `*_load_module` as appropriate.\n"
    "- Verify with `jvm_verify`/`llvm_verify`. If a tactic is required, infer the solver (do not explain).\n"
)


# ---- User template: supplies code and file metadata ----
USER_TEMPLATE = (
    "File path: {filename}\n"
    "Language hint: {lang}\n"
    "Goal: Produce one *instruction* asking the assistant to write a specification or verification artifact for this file.\n"
    "Return ONLY a JSON object with keys: instruction, input, output. Set 'output' to an empty string.\n"
    "If you include an 'input', keep it very short or empty.\n\n"
    "Code excerpt:\n"
    "-----8<-----\n{code}\n-----8<-----\n"
)

class AlpacaRow(BaseModel):
    instruction: str
    input: str
    output: str

# ---- Response schema: forces Alpaca fields ----
ALPACA_SCHEMA = {
    "name": "AlpacaRow",
    "strict": True,
    "schema": {
        "type": "object",
        "required": ["instruction", "input", "output"],
        "properties": {
            "instruction": {"type": "string"},
            "input": {"type": "string"},
            "output": {"type": "string"},
        },
        "additionalProperties": False,
    },
}

# Light language mapping for context
LANG_HINT = {
    ".cry": "Cryptol",
    ".saw": "SAWScript",
    ".c": "C",
    ".h": "C",
    ".py": "Python",
    ".rs": "Rust",
    ".cpp": "C++",
    ".hpp": "C++",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
}

# Recommended: keep this focused on your spec/verification tasks
DEFAULT_SYSTEM = (
    "You are a meticulous assistant that writes formal specifications and "
    "verification harnesses (Cryptol properties, SAW llvm_verify scripts, "
    "equivalence proofs). Follow the user's instruction exactly; do not include "
    "explanations unless asked."
)

def _build_user_content(instruction: str, alpaca_input: str, filename: Optional[str]) -> str:
    parts = [f"{instruction.strip()}"]
    #if filename:
    #    parts.append(f"### File:\n{filename}")
    if alpaca_input and alpaca_input.strip():
        parts.append(f"`{alpaca_input.strip()}`")
    return "\n".join(parts)

def alpaca_df_to_qwen_messages(
    df: pd.DataFrame,
    output: str,
    #system_prompt: str = DEFAULT_SYSTEM,
    drop_empty_output: bool = True,
    include_filename_in_user: bool = True,
) -> pd.DataFrame:
    """
    Convert a DataFrame with columns ['instruction','input','output', 'filename'?]
    into rows of {'messages': [{role,content}...]} for Qwen chat SFT.
    """
    records = []
    for _, r in df.iterrows():
        out = f"```{r['filetype']}\n{(r.get(output) or "").strip()}\n```"
        if drop_empty_output and not out:
            # SFT needs target text; skip empty outputs by default
            continue
        user = _build_user_content(
            instruction=r["instruction"],
            alpaca_input=r.get("input", ""),
            filename=r["filename"] if (include_filename_in_user and "filename" in r) else None,
        )
        messages = [
            {"role": "system", "content": \
             SYSTEM_PROMPT_CRYPTOL if r['filetype'] == "cryptol" else \
             SYSTEM_PROMPT_SAW if r['filetype'] == "saw" else ""},
            {"role": "user", "content": user},
            {"role": "assistant", "content": out},
        ]
        rec = {"messages": messages}
        # keep useful metadata alongside
        for k in ("filename", "filetype"):
            if k in r:
                rec[k] = r[k]
        records.append(rec)
    return pd.DataFrame(records)

def write_jsonl(rows: Iterable[Dict[str, Any]], path: str) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

# ----- CLI helpers (optional) -----
def from_alpaca_jsonl_to_qwen_messages(in_path: str, out_path: str, **kwargs) -> None:
    """
    Read an Alpaca JSONL with fields {instruction,input,output,(filename?),(filetype?)}
    and write Qwen chat messages JSONL.
    """
    df = pd.read_json(in_path, orient="records", lines=True)
    chat_df = alpaca_df_to_qwen_messages(df, **kwargs)
    write_jsonl(chat_df.to_dict(orient="records"), out_path)

def build_user_prompt(filename: str, lang: str, code: str, input_mode: str) -> Tuple[str, str]:
    if input_mode == "none":
        alpaca_input = ""
        code_for_prompt = code
    elif input_mode == "excerpt":
        alpaca_input = code[:512]
        code_for_prompt = code
    else:  
        alpaca_input = code
        code_for_prompt = code

    user = USER_TEMPLATE.format(filename=filename, lang=lang, code=code_for_prompt)
    return user, alpaca_input

def call_openai_structured(
        model: str, 
        system: str, 
        user: str,
        source_code: str = "",
        ) -> Dict[str, Any]:
    """
    Calls the OpenAI Responses API using JSON Schema 'response_format'
    to force exactly {instruction, input, output}.
    Retries with exponential backoff on transient errors.
    """
    from openai import OpenAI
    client = OpenAI()

    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.responses.parse(
            model=model,
            input=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user},
                ],
                text_format=AlpacaRow,   # << works here
                temperature=0.2,
                max_output_tokens=400,
            )
            alpaca_row = response.output_parsed.model_dump()
            if source_code:
                alpaca_row['instruction'] += f"\n\n# Source code:\n{source_code}"
            return alpaca_row
        except Exception as e:
            last_err = e
            if attempt == MAX_RETRIES:
                raise
            sleep_s = BACKOFF_BASE ** (attempt - 1)
            time.sleep(sleep_s)
    raise last_err  # should not get here

def build_prompt_call_openai_structured(
    model: str,
    input_mode: str,
    filename: str,
    lang: str,
    code: str,
) -> Dict[str, Any]:
    system_prompt = ""
    if lang == "cryptol":
        user, _ = build_user_prompt(filename, lang, code, input_mode=input_mode)
        system_prompt = sft_cryptol.SYSTEM_PROMPT_CRYPTOL
        result = call_openai_structured(model, system_prompt, user)
        return result
    else:
        user, source_code, _ = sft_saw.build_user_prompt(filename, code)
        system_prompt = sft_saw.SYSTEM_PROMPT
        result = call_openai_structured(model, system_prompt, user, source_code=source_code)
        return result

def iter_call_openai_structured(
    input_df: pd.DataFrame,
    model: str,
    input_mode: str,
    file_cache_path: str
) -> Iterable[Dict[str, Any]]:
    fileKVCache = FileKVCache(file_cache_path)
    returned_rows = []
    for _, row in input_df.iterrows():
        result = fileKVCache.get_or_call(
            row['filename'],
            build_prompt_call_openai_structured,
            {
                "model": model,
                "input_mode": input_mode,
                "filename": row['filename'],
                "lang": row['filetype'],
                "code": row['content'],
            }
        )
        returned_rows.append({
            "filename": row['filename'],
            "filetype": row['filetype'],
            **result,
            "content": row['content'],
        })
    return pd.DataFrame(returned_rows)

if __name__ == "__main__":
    pass