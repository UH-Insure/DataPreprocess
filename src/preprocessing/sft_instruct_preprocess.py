#!/usr/bin/env python3
"""
make_alpaca_instructions.py

Generate Alpaca-format rows (instruction/input/output) from source code files
by asking the OpenAI API to write a *spec-writing* instruction for each file.

Output: JSONL with fields:
  - instruction : str   (generated by the model; spec-writing style)
  - input       : str   (empty or a trimmed excerpt of the code)
  - output      : str   (left empty "" so you can fill later)

Optionally, generate a Batch API requests.jsonl for large-scale offline runs.

Usage:
  python make_alpaca_instructions.py \
      --src ./my_sources \
      --out alpaca_instructions.jsonl \
      --model gpt-4.1-mini \
      --include-ext .cry .saw .c .h \
      --input-mode excerpt \
      --max-chars 6000 \
      --batch-file requests.jsonl   # optional: create Batch API jobs file
"""
import re
import tiktoken
import argparse
import json
import os
import dotenv
import sys
import time
from pathlib import Path
from typing import Iterable, List, Dict, Any, Tuple, Optional
import pandas as pd
from pydantic import BaseModel
from src.util.file_kv_cache import FileKVCache
from . import sft_cryptol
from . import sft_saw
dotenv.load_dotenv()
# -------- Configurable defaults --------
DEFAULT_EXTS = [".cry", ".saw", ".c", ".h"]
DEFAULT_MODEL = "gpt-4.1-mini"
MAX_RETRIES = 2
BACKOFF_BASE = 1.8
CRYPTOL_VECTOR_STORE_ID = dotenv.get_key("CRYPTOL_VECTOR_STORE_ID", "")
# Heuristic: keep code snippet small so prompt stays within token budget.
DEFAULT_MAX_CHARS = 6000

# ---- System prompt tuned for *spec-writing* instructions ----
SYSTEM_PROMPT = (
    "You write *spec-writing instructions* for software verification or specification tasks.\n"
    "Given a code snippet, return exactly one instruction suitable for the Alpaca dataset.\n"
    "The instruction should ask a model to *write specifications* or *verification script* or *write an implementation*, e.g.,\n"
    "  - define a property that verifies the behavior of the values within the code snippet.\n"
    "  - write an implementation of the algorithm in Cryptol,\n"
    "  - write a SAWScript llvm_verify that verifies the implementation against the Cryptol specification,\n"
    "  - prove equivalence between two implementations, etc.\n"
    "The instruction must be standalone, concise (<= 150 words), and contain no solution.\n"
)

SYSTEM_PROMPT_CRYPTOL = (
    "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n"
)

SYSTEM_PROMPT_SAW = (
    "Return exactly ONE fenced code block labeled `saw` and nothing else (no prose before/after).\n"
)

# ---- User template: supplies code and file metadata ----
USER_TEMPLATE = (
    "File path: {filename}\n"
    "Language hint: {lang}\n"
    "Goal: Produce one *instruction* asking the assistant to write a specification or verification artifact for this file.\n"
    "Return ONLY a JSON object with keys: instruction, input, output. Set 'output' to an empty string.\n"
    "If you include an 'input', keep it very short or empty.\n\n"
    "Code excerpt:\n"
    "-----8<-----\n{code}\n-----8<-----\n"
)

class AlpacaRow(BaseModel):
    instruction: str
    input: str
    output: str

# ---- Response schema: forces Alpaca fields ----
ALPACA_SCHEMA = {
    "name": "AlpacaRow",
    "strict": True,
    "schema": {
        "type": "object",
        "required": ["instruction", "input", "output"],
        "properties": {
            "instruction": {"type": "string"},
            "input": {"type": "string"},
            "output": {"type": "string"},
        },
        "additionalProperties": False,
    },
}

TOKEN_LIMITS = {
    "gpt-5.1": 30000,
}
# Light language mapping for context
LANG_HINT = {
    ".cry": "Cryptol",
    ".saw": "SAWScript",
    ".c": "C",
    ".h": "C",
    ".py": "Python",
    ".rs": "Rust",
    ".cpp": "C++",
    ".hpp": "C++",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
}

# Recommended: keep this focused on your spec/verification tasks
DEFAULT_SYSTEM = (
    "You are a meticulous assistant that writes formal specifications and "
    "verification harnesses (Cryptol properties, SAW llvm_verify scripts, "
    "equivalence proofs). Follow the user's instruction exactly; do not include "
    "explanations unless asked."
)
enc = tiktoken.get_encoding("o200k_base")
def count_tokens_for_messages(messages):
    # Simple approximate scheme: concatenate role + content.
    # For exact ChatML accounting there is some small overhead, but this is
    # good enough for routing.
    text_chunks = []
    for m in messages:
        text_chunks.append(m["role"])
        text_chunks.append(m["content"])
    full_text = "\n".join(text_chunks)

    tokens = enc.encode(full_text)
    return len(tokens)

def _build_user_content(instruction: str, alpaca_input: str, filename: Optional[str]) -> str:
    parts = [f"{instruction.strip()}"]
    #if filename:
    #    parts.append(f"### File:\n{filename}")
    if alpaca_input and alpaca_input.strip():
        parts.append(f"`{alpaca_input.strip()}`")
    return "\n".join(parts)

def alpaca_df_to_qwen_messages(
    df: pd.DataFrame,
    output: str,
    #system_prompt: str = DEFAULT_SYSTEM,
    drop_input: bool = True,
    include_filename_in_user: bool = True,
) -> pd.DataFrame:
    """
    Convert a DataFrame with columns ['instruction','input','output', 'filename'?]
    into rows of {'messages': [{role,content}...]} for Qwen chat SFT.
    """
    records = []
    for _, r in df.iterrows():
        out = f"```{r['filetype']}\n{(r.get(output) or "").strip()}\n```"
        user = _build_user_content(
            instruction=r["instruction"],
            alpaca_input= None if drop_input else r.get("input", ""),
            filename=r["filename"] if (include_filename_in_user and "filename" in r) else None,
        )
        messages = [
            {"role": "system", "content": \
             SYSTEM_PROMPT_CRYPTOL if r['filetype'] == "cryptol" else \
             SYSTEM_PROMPT_SAW if r['filetype'] == "saw" else ""},
            {"role": "user", "content": user},
            {"role": "assistant", "content": out},
        ]
        rec = {"messages": messages}
        # keep useful metadata alongside
        for k in ("filename", "filetype", "set"):
            if k in r:
                rec[k] = r[k]
        records.append(rec)
    return pd.DataFrame(records)

def write_jsonl(rows: Iterable[Dict[str, Any]], path: str) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

# ----- CLI helpers (optional) -----
def from_alpaca_jsonl_to_qwen_messages(in_path: str, out_path: str, **kwargs) -> None:
    """
    Read an Alpaca JSONL with fields {instruction,input,output,(filename?),(filetype?)}
    and write Qwen chat messages JSONL.
    """
    df = pd.read_json(in_path, orient="records", lines=True)
    chat_df = alpaca_df_to_qwen_messages(df, **kwargs)
    write_jsonl(chat_df.to_dict(orient="records"), out_path)

def build_user_prompt(filename: str, lang: str, code: str, input_mode: str) -> Tuple[str, str]:
    if input_mode == "none":
        alpaca_input = ""
        code_for_prompt = code
    elif input_mode == "excerpt":
        alpaca_input = code[:512]
        code_for_prompt = code
    else:  
        alpaca_input = code
        code_for_prompt = code

    user = USER_TEMPLATE.format(filename=filename, lang=lang, code=code_for_prompt)
    return user, alpaca_input

def call_openai_structured(
        model: str, 
        system: str, 
        user: str,
        source_code: str = "",
        ) -> Dict[str, Any]:
    """
    Calls the OpenAI Responses API using JSON Schema 'response_format'
    to force exactly {instruction, input, output}.
    Retries with exponential backoff on transient errors.
    """
    from openai import OpenAI, RateLimitError
    client = OpenAI()
    tools = [{
        "type": "file_search",
        "vector_store_ids": ["vs_691cd78f3e088191a660732e83652938"],
        # optional: limit number of retrieved chunks
        "max_num_results": 3,
    }]
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            tokens = count_tokens_for_messages(
                [
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
            )
            response = client.responses.parse(
            model= "gpt-5-2025-08-07" if model in TOKEN_LIMITS.keys() and tokens > TOKEN_LIMITS[model] else model,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
                ],
                text_format=AlpacaRow,   # << works here
                #temperature=0.2,
                #max_output_tokens=400,
                tools=tools,
            )
            print(f"Response:\n\n{response}")
            alpaca_row = response.output_parsed.model_dump()
            if source_code:
                alpaca_row['instruction'] += f"\n\n# Source code:\n{source_code}"
            return alpaca_row
        except RateLimitError as e:
            # --- 1) Try Retry-After header if present ------------------------
            wait_seconds = None
            try:
                # e.response is an httpx.Response under the hood
                retry_after = e.response.headers.get("retry-after") if e.response else None
                if retry_after is not None:
                    wait_seconds = float(retry_after)
            except Exception:
                pass

            # --- 2) Fallback: parse "Please try again in 7.368s." ------------
            if wait_seconds is None:
                m = re.search(r"Please try again in ([0-9.]+)s", str(e))
                if m:
                    time.sleep(1)
                    wait_seconds = float(m.group(1))

            # --- 3) Last resort: exponential backoff -------------------------
            if wait_seconds is None:
                wait_seconds = min(60, 2 ** attempt)

            print(f"Rate limit hit: {e}")
            print(f"Sleeping for {wait_seconds:.2f} seconds before retry #{attempt+1}...")
            time.sleep(wait_seconds)
        except Exception as e:
            last_err = e
            if attempt == MAX_RETRIES:
                raise
            sleep_s = BACKOFF_BASE ** (attempt - 1)
            time.sleep(sleep_s)
    raise last_err  # should not get here

def build_prompt_call_openai_structured(
    model: str,
    input_mode: str,
    filename: str,
    lang: str,
    code: str,
) -> Dict[str, Any]:
    system_prompt = ""
    if lang == "cryptol":
        user, _ = build_user_prompt(filename, lang, code, input_mode=input_mode)
        system_prompt = sft_cryptol.SYSTEM_PROMPT_CRYPTOL
        result = call_openai_structured(model, system_prompt, user)
        return result
    else:
        user, source_code, _ = sft_saw.build_user_prompt(filename, code)
        system_prompt = sft_saw.SYSTEM_PROMPT
        result = call_openai_structured(model, system_prompt, user, source_code=source_code)
        return result

def iter_call_openai_structured(
    input_df: pd.DataFrame,
    model: str,
    input_mode: str,
    file_cache_path: str
) -> Iterable[Dict[str, Any]]:
    fileKVCache = FileKVCache(file_cache_path)
    returned_rows = []
    for _, row in input_df.iterrows():
        result = fileKVCache.get_or_call(
            row['filename'],
            build_prompt_call_openai_structured,
            {
                "model": model,
                "input_mode": input_mode,
                "filename": row['filename'],
                "lang": row['filetype'],
                "code": row['content'],
            }
        )
        returned_rows.append({
            "filename": row['filename'],
            "filetype": row['filetype'],
            "set": row['set'],
            **result,
            "content": row['content'],
        })
    return pd.DataFrame(returned_rows)

if __name__ == "__main__":
    pass