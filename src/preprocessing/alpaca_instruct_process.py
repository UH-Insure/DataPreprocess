#!/usr/bin/env python3
"""
make_alpaca_instructions.py

Generate Alpaca-format rows (instruction/input/output) from source code files
by asking the OpenAI API to write a *spec-writing* instruction for each file.

Output: JSONL with fields:
  - instruction : str   (generated by the model; spec-writing style)
  - input       : str   (empty or a trimmed excerpt of the code)
  - output      : str   (left empty "" so you can fill later)

Optionally, generate a Batch API requests.jsonl for large-scale offline runs.

Usage:
  python make_alpaca_instructions.py \
      --src ./my_sources \
      --out alpaca_instructions.jsonl \
      --model gpt-4.1-mini \
      --include-ext .cry .saw .c .h \
      --input-mode excerpt \
      --max-chars 6000 \
      --batch-file requests.jsonl   # optional: create Batch API jobs file
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Iterable, List, Dict, Any, Tuple
import pandas as pd
from pydantic import BaseModel
from src.util.file_kv_cache import FileKVCache

# -------- Configurable defaults --------
DEFAULT_EXTS = [".cry", ".saw", ".c", ".h"]
DEFAULT_MODEL = "gpt-4.1-mini"
MAX_RETRIES = 2
BACKOFF_BASE = 1.8

# Heuristic: keep code snippet small so prompt stays within token budget.
DEFAULT_MAX_CHARS = 6000

# ---- System prompt tuned for *spec-writing* instructions ----
SYSTEM_PROMPT = (
    "You write *spec-writing instructions* for software verification or specification tasks.\n"
    "Given a code snippet, return exactly one instruction suitable for the Alpaca dataset.\n"
    "The instruction should ask a model to *write specifications* or *verification harnesses*, e.g.,\n"
    "  - write Cryptol properties suitable for :prove / :check,\n"
    "  - write a SAWScript llvm_verify harness for a function,\n"
    "  - prove equivalence between two implementations, etc.\n"
    "The instruction must be standalone, concise (<= 150 words), and contain no solution.\n"
)

# ---- User template: supplies code and file metadata ----
USER_TEMPLATE = (
    "File path: {filename}\n"
    "Language hint: {lang}\n"
    "Goal: Produce one *instruction* asking the assistant to write a specification or verification artifact for this file.\n"
    "Return ONLY a JSON object with keys: instruction, input, output. Set 'output' to an empty string.\n"
    "If you include an 'input', keep it very short or empty.\n\n"
    "Code excerpt:\n"
    "-----8<-----\n{code}\n-----8<-----\n"
)

class AlpacaRow(BaseModel):
    instruction: str
    input: str
    output: str

# ---- Response schema: forces Alpaca fields ----
ALPACA_SCHEMA = {
    "name": "AlpacaRow",
    "strict": True,
    "schema": {
        "type": "object",
        "required": ["instruction", "input", "output"],
        "properties": {
            "instruction": {"type": "string"},
            "input": {"type": "string"},
            "output": {"type": "string"},
        },
        "additionalProperties": False,
    },
}

# Light language mapping for context
LANG_HINT = {
    ".cry": "Cryptol",
    ".saw": "SAWScript",
    ".c": "C",
    ".h": "C",
    ".py": "Python",
    ".rs": "Rust",
    ".cpp": "C++",
    ".hpp": "C++",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
}

def build_user_prompt(filename: str, lang: str, code: str, input_mode: str) -> Tuple[str, str]:
    if input_mode == "none":
        alpaca_input = ""
        code_for_prompt = code
    elif input_mode == "excerpt":
        alpaca_input = code[:512]
        code_for_prompt = code
    else:  
        alpaca_input = code
        code_for_prompt = code

    user = USER_TEMPLATE.format(filename=filename, lang=lang, code=code_for_prompt)
    return user, alpaca_input

def call_openai_structured(model: str, system: str, user: str) -> Dict[str, Any]:
    """
    Calls the OpenAI Responses API using JSON Schema 'response_format'
    to force exactly {instruction, input, output}.
    Retries with exponential backoff on transient errors.
    """
    from openai import OpenAI
    client = OpenAI()

    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.responses.parse(
            model=model,
            input=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user},
                ],
                text_format=AlpacaRow,   # << works here
                temperature=0.2,
                max_output_tokens=400,
            )
            alpaca_row = response.output_parsed.model_dump()
            return alpaca_row
        except Exception as e:
            last_err = e
            if attempt == MAX_RETRIES:
                raise
            sleep_s = BACKOFF_BASE ** (attempt - 1)
            time.sleep(sleep_s)
    raise last_err  # should not get here

def build_prompt_call_openai_structured(
    model: str,
    input_mode: str,
    filename: str,
    lang: str,
    code: str,
) -> Dict[str, Any]:
    user, _ = build_user_prompt(filename, lang, code, input_mode=input_mode)
    result = call_openai_structured(model, SYSTEM_PROMPT, user)
    return result

def iter_call_openai_structured(
    input_df: pd.DataFrame,
    model: str,
    input_mode: str,
    file_cache_path: str
) -> Iterable[Dict[str, Any]]:
    fileKVCache = FileKVCache(file_cache_path)
    returned_rows = []
    for _, row in input_df.iterrows():
        result = fileKVCache.get_or_call(
            row['filename'],
            build_prompt_call_openai_structured,
            {
                "model": model,
                "input_mode": input_mode,
                "filename": row['filename'],
                "lang": row['filetype'],
                "code": row['content'],
            }
        )
        returned_rows.append({
            "filename": row['filename'],
            "filetype": row['filetype'],
            **result,
            "content": row['content'],
        })
    return pd.DataFrame(returned_rows)

if __name__ == "__main__":
    pass