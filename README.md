# DataPreprocess
This repository contains the modules used to preprocess the raw Cryptol data obtained from various sources such as GitHub repositories and online articles. To use this repo:
1. Collect raw data sources.
    - Pull all the repositories that will be used for training.
    - Scrape all the raw text sources you desire from the web.
        - This project was lucky, as all the desired text sources were able to copied from the rendered web page and no HTML processing was neccesary.
2. Run `src.util.code_parser.py` 
3. Run `src.util.text_parser.py`
4. Run the notebooks in increasing order 01-06.
    - Details about the notebooks can be found in the notebooks/ directory.
## Preprocessing Pipeline
Repository provides a complete pipeline for:
- Recursively crawling multiple repositories
- Extracting Cryptol and SAW source files
- Cleaning and normalizing the files
- Pairing source code with appropriate instruction prompts
- Generating Supervised Fine-Tuning (SFT) datasets in chat format
- Producing evaluation suites used to measure model performance
- Caching filesystem queries to accelerate repeated processing
The repository is organized as a modular preprocessing toolkit that supports deterministic, repeatable dataset creation â€” suitable for research, experiments, and training large language models.

## Repository Structure
- **DataPreprocess/**
  - main.py
  - **notebooks/**
    - Evals_Monolithic.ipynb
    - Evals_Small.ipynb
  - **src/**
    - **preprocessing/**
      - dataset_builder.py
      - remove_copyrights.py
      - sft_instruct_preprocess.py
      - text_agent.py
    - **eval/**
      - eval_suite.py
    - **util/**
      - file_kv_cache.py
      - text_parser.py
  - **data/**
    - (output .jsonl files generated by scripts)

