# DataPreprocess
This project for builds training and evaluation datasets for machine-learning models that generate or analyze formal specification languages such as Cryptol and SAWScript.
It provides a complete pipeline for:
- Recursively crawling multiple repositories
- Extracting Cryptol and SAW source files
- Cleaning and normalizing the files
- Pairing source code with appropriate instruction prompts
- Generating Supervised Fine-Tuning (SFT) datasets in chat format
- Producing evaluation suites used to measure model performance
- Caching filesystem queries to accelerate repeated processing
The repository is organized as a modular preprocessing toolkit that supports deterministic, repeatable dataset creation â€” suitable for research, experiments, and training large language models.

## Repository Structure
- **DataPreprocess/**
  - main.py
  - **notebooks/**
    - Evals_Monolithic.ipynb
    - Evals_Small.ipynb
  - **src/**
    - **preprocessing/**
      - dataset_builder.py
      - remove_copyrights.py
      - sft_instruct_preprocess.py
      - text_agent.py
    - **eval/**
      - eval_suite.py
    - **util/**
      - file_kv_cache.py
      - text_parser.py
  - **data/**
    - (output .jsonl files generated by scripts)

