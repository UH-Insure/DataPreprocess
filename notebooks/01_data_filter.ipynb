{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ec3da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.chdir(Path(os.getenv(\"PYTHONPATH\")).expanduser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d2f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.preprocessing.quality_process import compute_file_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4302bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 rows\n"
     ]
    }
   ],
   "source": [
    "# Source code location\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol-specs\n",
    "# /Users/josh/SecurityAnalytics/development/saw-script\n",
    "DATA = 'all_slices_raw'\n",
    "# --- paths ---\n",
    "jsonl_path = f\"data/{DATA}.jsonl\"      # your input dataset\n",
    "\n",
    "# --- load dataset ---\n",
    "rows = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(rows)} rows\")\n",
    "\n",
    "# --- run quality_process.py for each row ---\n",
    "results = []\n",
    "for row in rows:\n",
    "    results.append(\n",
    "        compute_file_metrics(\n",
    "            row[\"filename\"],\n",
    "            row[\"content\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- save to CSV ---\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a03ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/j77njgmn2ts9wkln0x_x_vcc0000gn/T/ipykernel_31949/3775316867.py:50: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n"
     ]
    }
   ],
   "source": [
    "# StarCoder-like thresholds (tune if needed)\n",
    "MAX_BYTES         = 200_000\n",
    "MAX_NONASCII      = 0.20\n",
    "ENC_MAX_RUN_CHARS = 1024\n",
    "ENC_MAX_FRACTION  = 0.50\n",
    "MAX_LINES_TOTAL   = 100_000\n",
    "MAX_LINE_AVG_LEN  = 100\n",
    "MAX_LINE_MAX_LEN  = 1_000\n",
    "MIN_TOKENS_LANG   = 40      # language-token gate (Cryptol tokenizer)\n",
    "MAX_TOKENS_LANG   = 10_000  # optional upper bound\n",
    "MIN_TOKENS_MODEL  = 25      # only if youâ€™ve populated num_tokens_model\n",
    "MAX_HEXNUM_RATIO  = 0.20\n",
    "\n",
    "\n",
    "# --- exact dedup (keep first occurrence of each sha1) ---\n",
    "# mark duplicates (True means \"is duplicate\" => drop later)\n",
    "dup_mask = df.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
    "\n",
    "# --- encoded data (StarCoder) ---\n",
    "enc_mask = (df[\"enc_max_run\"] > ENC_MAX_RUN_CHARS) | (df[\"enc_fraction\"] > ENC_MAX_FRACTION)\n",
    "\n",
    "# --- long-line filters (StarCoder) ---\n",
    "longline_mask = (\n",
    "    (df[\"lines\"] > MAX_LINES_TOTAL) |\n",
    "    (df[\"avg_line_len\"] > MAX_LINE_AVG_LEN) |\n",
    "    (df[\"max_line_len\"] > MAX_LINE_MAX_LEN)\n",
    ")\n",
    "\n",
    "# --- binary-like content ---\n",
    "binary_mask = df[\"binary_like\"].fillna(False)\n",
    "\n",
    "# --- non-ascii density ---\n",
    "nonascii_mask = df[\"non_ascii_ratio\"].fillna(0) > MAX_NONASCII\n",
    "\n",
    "# --- size guardrail (bytes) ---\n",
    "bytes_mask = df[\"bytes\"].fillna(0) > MAX_BYTES\n",
    "\n",
    "# --- language-token bounds ---\n",
    "lang_small_mask = df[\"num_tokens_lang\"].fillna(0) < MIN_TOKENS_LANG\n",
    "lang_large_mask = df[\"num_tokens_lang\"].fillna(0) > MAX_TOKENS_LANG\n",
    "\n",
    "# --- shingles exist (needed for Jaccard) ---\n",
    "no_shingles_mask = df[\"num_shingles\"].fillna(0) <= 0\n",
    "\n",
    "# --- numeric/hex blob concentration ---\n",
    "hexnum_mask = df[\"hexnum_ratio\"].fillna(0) > MAX_HEXNUM_RATIO\n",
    "\n",
    "# --- model-token gate (only apply where available) ---\n",
    "if \"num_tokens_model\" in df.columns:\n",
    "    model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n",
    "else:\n",
    "    model_small_mask = pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c71bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all hard-drop reasons\n",
    "drop_mask = (\n",
    "    dup_mask |\n",
    "    enc_mask |\n",
    "    longline_mask |\n",
    "    binary_mask |\n",
    "    nonascii_mask |\n",
    "    bytes_mask |\n",
    "    lang_small_mask |\n",
    "    lang_large_mask |\n",
    "    no_shingles_mask |\n",
    "    hexnum_mask |\n",
    "    model_small_mask\n",
    ")\n",
    "\n",
    "# Optional: compute a human-readable fail reason (first rule that tripped)\n",
    "def first_reason(i):\n",
    "    if dup_mask.iat[i]:          return \"exact_duplicate\"\n",
    "    if enc_mask.iat[i]:          return \"encoded_data\"\n",
    "    if longline_mask.iat[i]:     return \"long_lines\"\n",
    "    if binary_mask.iat[i]:       return \"binary_like\"\n",
    "    if nonascii_mask.iat[i]:     return \"too_much_nonascii\"\n",
    "    if bytes_mask.iat[i]:        return \"too_large_bytes\"\n",
    "    if lang_small_mask.iat[i]:   return \"too_few_lang_tokens\"\n",
    "    if lang_large_mask.iat[i]:   return \"too_many_lang_tokens\"\n",
    "    if no_shingles_mask.iat[i]:  return \"no_shingles\"\n",
    "    if hexnum_mask.iat[i]:       return \"hexnum_blob\"\n",
    "    if model_small_mask.iat[i]:  return \"too_few_model_tokens\"\n",
    "    return \"ok\"\n",
    "\n",
    "df = df.copy()\n",
    "df[\"quality_ok\"] = ~drop_mask\n",
    "df[\"fail_reason\"] = [first_reason(i) for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a954212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif put_back_path.exists():\\n    put_back_set = pd.read_csv(put_back_path)\\n    put_back_filenames = set(put_back_set[\"filename\"].dropna().tolist())\\nelse:\\n    put_back_filenames = set()\\n\\nfor fname in put_back_filenames:\\n    if fname in df[\\'filename\\'].values:\\n        candidate_df = pd.concat([candidate_df, df[df[\\'filename\\'] == fname][dedup_cols]], ignore_index=True)\\n        candidate_df.loc[candidate_df[\\'filename\\'] == fname, \\'quality_ok\\'] = True\\n        '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup_cols = [\n",
    "    \"filename\", \"sha1\",\n",
    "    # size/lines\n",
    "    \"bytes\", \"lines\", \"avg_line_len\", \"max_line_len\",\n",
    "    # content/encoding\n",
    "    \"non_ascii_ratio\", \"binary_like\",\n",
    "    \"enc_total_matched\", \"enc_max_run\", \"enc_fraction\",\n",
    "    \"enc_hits_base64\", \"enc_hits_hexbytes\", \"enc_hits_unicode\",\n",
    "    # tokens/shingles\n",
    "    \"num_tokens_lang\", \"k_shingle\", \"num_shingles\", \"hexnum_ratio\",\n",
    "    # model tokens (optional)\n",
    "    \"num_tokens_model\",\n",
    "    # path heuristic & status\n",
    "    \"quality_ok\", \"fail_reason\",\n",
    "]\n",
    "\n",
    "candidate_df = df.loc[df[\"quality_ok\"], dedup_cols].reset_index(drop=True)\n",
    "put_back_path = Path(\"data/dropped/files_to_put_back.csv\")\n",
    "'''\n",
    "if put_back_path.exists():\n",
    "    put_back_set = pd.read_csv(put_back_path)\n",
    "    put_back_filenames = set(put_back_set[\"filename\"].dropna().tolist())\n",
    "else:\n",
    "    put_back_filenames = set()\n",
    "\n",
    "for fname in put_back_filenames:\n",
    "    if fname in df['filename'].values:\n",
    "        candidate_df = pd.concat([candidate_df, df[df['filename'] == fname][dedup_cols]], ignore_index=True)\n",
    "        candidate_df.loc[candidate_df['filename'] == fname, 'quality_ok'] = True\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0156792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[summary] total: 1000\n",
      "[summary] kept : 507\n",
      "[summary] dropped: 493\n",
      "[summary] drop reasons:\n",
      "fail_reason\n",
      "exact_duplicate        314\n",
      "too_few_lang_tokens    166\n",
      "hexnum_blob              9\n",
      "encoded_data             4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[summary] total:\", len(df))\n",
    "print(\"[summary] kept :\", int(df[\"quality_ok\"].sum()))\n",
    "print(\"[summary] dropped:\", int((~df[\"quality_ok\"]).sum()))\n",
    "print(\"[summary] drop reasons:\")\n",
    "print(df.loc[~df[\"quality_ok\"], \"fail_reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c12ceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = df[df[\"quality_ok\"] == False].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9846b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_set = df[~df['fail_reason'].isin(['ok', 'exact_duplicate'])].copy().reset_index(drop=True)\n",
    "out_path = Path(\"data/dropped/review_files.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "review_data_set.to_csv(f\"data/dropped/review_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec151e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] ==== Starting MinHash/LSH over DataFrame ====\n",
      "[info] params: K_SHINGLE=5, NUM_PERM=512, LSH_THRESHOLD=0.7\n",
      "[info] loaded 507 files from candidate_df\n",
      "[info] files indexed   : 507\n",
      "[diag] total candidate pairs: 939\n",
      "[diag] pairs with jaccard >= 0.7: 728\n",
      "[info] wrote CSV and Parquet to minhash_outputs/\n",
      "\n",
      "[info] ==== MinHash/LSH run summary ====\n",
      "[info] files loaded  : 507\n",
      "[info] files indexed : 507\n",
      "[info] files with 0 shingles (tokens < 5): 0\n",
      "[info] candidate pairs (from LSH) : 939\n",
      "[info] pairs with Jaccard >= 0.60: 778\n",
      "[info] pairs with Jaccard >= 0.70: 728\n",
      "[info] pairs with Jaccard >= 0.80: 548\n",
      "[info] pairs with Jaccard >= 0.85: 484\n",
      "[info] pairs with Jaccard >= 0.90: 371\n",
      "[info] avg Jaccard (candidates)  : 0.8046\n",
      "[info] max Jaccard               : 0.9947\n",
      "[info] min Jaccard               : 0.5444\n",
      "\n",
      "[info] top pairs:\n",
      "                                                                                                        a                                                                                                                b  jaccard  a_shingles  b_shingles  union_shingles  intersect_shingles\n",
      "                             cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/067_katantanEncrypt.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/090_katantan64_1.cry 0.994678        1869        1879            1879                1869\n",
      "                             cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/067_katantanEncrypt.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/091_katantan64_2.cry 0.994678        1869        1879            1879                1869\n",
      "                                     cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/086_tests64.cry                                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/087_testsPass.cry 0.994138        2039        2043            2047                2035\n",
      "                       aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Block/DES/001_DES.cry                        aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Block/DES/054_DESCorrect.cry 0.991057        1884        1901            1901                1884\n",
      "                                cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/036_katanEncrypt.cry                                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/088_katan64_1.cry 0.989785         969         979             979                 969\n",
      "                                cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/036_katanEncrypt.cry                                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/089_katan64_2.cry 0.989785         969         979             979                 969\n",
      "                                cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/090_katantan64_1.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/091_katantan64_2.cry 0.989412        1879        1879            1889                1869\n",
      "                             cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/067_katantanEncrypt.cry                                   cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/069_katantanEncrypt64.cry 0.988366        1869        1891            1891                1869\n",
      "                        aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Stream/ZUC/014_S.cry                         aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Stream/ZUC/016_example8.cry 0.986051        1135        1143            1147                1131\n",
      "                      aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Stream/ZUC/035_ZUC.cry aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Stream/ZUC/046_ZUC_isResistantToCollisionAttack.cry 0.985586        1983        2012            2012                1983\n",
      "AES-GCM-SIV-proof/proof/asm/deps/saw-script/examples/openssl_aes/AES128TBox/037_aesEncryptKeySchedule.cry        AES-GCM-SIV-proof/proof/asm/deps/saw-script/examples/openssl_aes/AES128TBox/065_keyExpansionInjective.cry 0.983264         940         956             956                 940\n",
      "                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/069_katantanEncrypt64.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/090_katantan64_1.cry 0.983167        1891        1879            1901                1869\n",
      "                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/069_katantanEncrypt64.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/091_katantan64_2.cry 0.983167        1891        1879            1901                1869\n",
      "                             cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/067_katantanEncrypt.cry                                   cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/071_katantanEncrypt80.cry 0.983167        1869        1901            1901                1869\n",
      "                                   cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/088_katan64_1.cry                                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/089_katan64_2.cry 0.979778         979         979             989                 969\n",
      "                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/071_katantanEncrypt80.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/091_katantan64_2.cry 0.978022        1901        1879            1911                1869\n",
      "                           cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/071_katantanEncrypt80.cry                                        cryptol-specs/Primitive/Symmetric/Cipher/Block/KATAN/090_katantan64_1.cry 0.978022        1901        1879            1911                1869\n",
      "                 AES-GCM-SIV-proof/proof/asm/deps/saw-script/examples/openssl_aes/AES128TBox/020_sbox.cry                                        saw-demos/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/083_sbox.cry 0.977055         511         523             523                 511\n",
      "                        aws-lc-verification/cryptol-specs/McEliece_KEM/high-level/keccak/005_SHAKE128.cry                                  aws-lc-verification/cryptol-specs/McEliece_KEM/high-level/keccak/009_Keccak.cry 0.976998         826         807             826                 807\n",
      "                        aws-lc-verification/cryptol-specs/McEliece_KEM/high-level/keccak/006_SHAKE256.cry                                  aws-lc-verification/cryptol-specs/McEliece_KEM/high-level/keccak/009_Keccak.cry 0.976998         826         807             826                 807\n",
      "[info] saved hash signatures JSONL: minhash_outputs/minhash_signatures.jsonl\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.similiar_process import run_from_dataframe\n",
    "\n",
    "# candidate_df must have an absolute-path 'filename' column.\n",
    "df_files, df_pairs, similar_files = run_from_dataframe(\n",
    "    candidate_df,\n",
    "    filename_col=\"filename\",\n",
    "    root_dir=f\"{os.getenv(\"REPO_ROOT\")}/cryptol_slices\",  # prepended to filename when opening\n",
    "    out_dir=\"minhash_outputs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55aa4783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_shingles</th>\n",
       "      <th>num_perm</th>\n",
       "      <th>k_shingle</th>\n",
       "      <th>minhash_hashvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[174239792, 55431911, 89718616, 82978847, 9702...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[274167780, 95042772, 35365453, 260801999, 205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...</td>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[46376070, 67873997, 35365453, 106715415, 1380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[46376070, 67873997, 35365453, 80543433, 10160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...</td>\n",
       "      <td>515</td>\n",
       "      <td>511</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[4333910, 2871210, 1372024, 12196459, 659441, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  num_tokens  \\\n",
       "0  AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...          44   \n",
       "1  AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...          40   \n",
       "2  AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...          90   \n",
       "3  AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...         120   \n",
       "4  AES-GCM-SIV-proof/proof/asm/deps/saw-script/ex...         515   \n",
       "\n",
       "   num_shingles  num_perm  k_shingle  \\\n",
       "0            40       512          5   \n",
       "1            36       512          5   \n",
       "2            86       512          5   \n",
       "3           116       512          5   \n",
       "4           511       512          5   \n",
       "\n",
       "                                  minhash_hashvalues  \n",
       "0  [174239792, 55431911, 89718616, 82978847, 9702...  \n",
       "1  [274167780, 95042772, 35365453, 260801999, 205...  \n",
       "2  [46376070, 67873997, 35365453, 106715415, 1380...  \n",
       "3  [46376070, 67873997, 35365453, 80543433, 10160...  \n",
       "4  [4333910, 2871210, 1372024, 12196459, 659441, ...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aefc8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] clusters formed   : 306\n",
      "[info] kept files        : 306\n",
      "[info] dropped files     : 201\n",
      "[info] wrote keep/drop/cluster CSVs to minhash_outputs/\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.cluster_process import run_clustering\n",
    "\n",
    "# If you already have df_files/df_pairs in memory:\n",
    "df_keep, df_drop, df_clusters = run_clustering(\n",
    "    df_files=df_files,          # from similiar_process\n",
    "    df_pairs=df_pairs,          # from similiar_process\n",
    "    jaccard_keep_threshold=0.70,\n",
    "    out_dir=\"minhash_outputs\",\n",
    "    content_lookup=None,        # or {filename: raw_text} if you want text-derived penalties\n",
    "    save_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d94f0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[df['filename'].isin(df_keep['filename'].tolist())].copy()\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'quality_ok'\n",
    "    ] = False\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'fail_reason'\n",
    "    ] = 'similiar_file_exists'\n",
    "\n",
    "dropped = df[~df['filename'].isin(dataset['filename'].tolist())].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00b19640",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.to_csv(f\"data/dropped/{DATA}_dropped_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec52c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_files = set(dataset[\"filename\"].unique())\n",
    "\n",
    "all_files_df = pd.read_json(jsonl_path, lines=True)\n",
    "all_files_filtered_df = all_files_df[all_files_df[\"filename\"].isin(verified_files)].reset_index(drop=True)\n",
    "out_path = Path(f\"data/training_datasets/{DATA}_verified_nomods.jsonl\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "all_files_filtered_df.to_json(out_path, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6c3b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/{DATA}_file_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InsureHub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
