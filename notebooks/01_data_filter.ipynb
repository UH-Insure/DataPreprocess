{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec3da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.chdir(Path(os.getenv(\"PYTHONPATH\")).expanduser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.preprocessing.quality_process import compute_file_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4302bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3146 rows\n"
     ]
    }
   ],
   "source": [
    "# Source code location\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol-specs\n",
    "# /Users/josh/SecurityAnalytics/development/saw-script\n",
    "\n",
    "# --- paths ---\n",
    "jsonl_path = \"data/all_sources_raw.jsonl\"      # your input dataset\n",
    "\n",
    "# --- load dataset ---\n",
    "rows = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(rows)} rows\")\n",
    "\n",
    "# --- run quality_process.py for each row ---\n",
    "results = []\n",
    "for row in rows:\n",
    "    results.append(\n",
    "        compute_file_metrics(\n",
    "            row[\"filename\"],\n",
    "            row[\"content\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- save to CSV ---\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a03ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/j77njgmn2ts9wkln0x_x_vcc0000gn/T/ipykernel_44602/2718623724.py:50: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n"
     ]
    }
   ],
   "source": [
    "# StarCoder-like thresholds (tune if needed)\n",
    "MAX_BYTES         = 200_000\n",
    "MAX_NONASCII      = 0.20\n",
    "ENC_MAX_RUN_CHARS = 1024\n",
    "ENC_MAX_FRACTION  = 0.50\n",
    "MAX_LINES_TOTAL   = 100_000\n",
    "MAX_LINE_AVG_LEN  = 100\n",
    "MAX_LINE_MAX_LEN  = 1_000\n",
    "MIN_TOKENS_LANG   = 40      # language-token gate (Cryptol tokenizer)\n",
    "MAX_TOKENS_LANG   = 10_000  # optional upper bound\n",
    "MIN_TOKENS_MODEL  = 32      # only if youâ€™ve populated num_tokens_model\n",
    "MAX_HEXNUM_RATIO  = 0.20\n",
    "\n",
    "\n",
    "# --- exact dedup (keep first occurrence of each sha1) ---\n",
    "# mark duplicates (True means \"is duplicate\" => drop later)\n",
    "dup_mask = df.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
    "\n",
    "# --- encoded data (StarCoder) ---\n",
    "enc_mask = (df[\"enc_max_run\"] > ENC_MAX_RUN_CHARS) | (df[\"enc_fraction\"] > ENC_MAX_FRACTION)\n",
    "\n",
    "# --- long-line filters (StarCoder) ---\n",
    "longline_mask = (\n",
    "    (df[\"lines\"] > MAX_LINES_TOTAL) |\n",
    "    (df[\"avg_line_len\"] > MAX_LINE_AVG_LEN) |\n",
    "    (df[\"max_line_len\"] > MAX_LINE_MAX_LEN)\n",
    ")\n",
    "\n",
    "# --- binary-like content ---\n",
    "binary_mask = df[\"binary_like\"].fillna(False)\n",
    "\n",
    "# --- non-ascii density ---\n",
    "nonascii_mask = df[\"non_ascii_ratio\"].fillna(0) > MAX_NONASCII\n",
    "\n",
    "# --- size guardrail (bytes) ---\n",
    "bytes_mask = df[\"bytes\"].fillna(0) > MAX_BYTES\n",
    "\n",
    "# --- language-token bounds ---\n",
    "lang_small_mask = df[\"num_tokens_lang\"].fillna(0) < MIN_TOKENS_LANG\n",
    "lang_large_mask = df[\"num_tokens_lang\"].fillna(0) > MAX_TOKENS_LANG\n",
    "\n",
    "# --- shingles exist (needed for Jaccard) ---\n",
    "no_shingles_mask = df[\"num_shingles\"].fillna(0) <= 0\n",
    "\n",
    "# --- numeric/hex blob concentration ---\n",
    "hexnum_mask = df[\"hexnum_ratio\"].fillna(0) > MAX_HEXNUM_RATIO\n",
    "\n",
    "# --- model-token gate (only apply where available) ---\n",
    "if \"num_tokens_model\" in df.columns:\n",
    "    model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n",
    "else:\n",
    "    model_small_mask = pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all hard-drop reasons\n",
    "drop_mask = (\n",
    "    dup_mask |\n",
    "    enc_mask |\n",
    "    longline_mask |\n",
    "    binary_mask |\n",
    "    nonascii_mask |\n",
    "    bytes_mask |\n",
    "    lang_small_mask |\n",
    "    lang_large_mask |\n",
    "    no_shingles_mask |\n",
    "    hexnum_mask |\n",
    "    model_small_mask\n",
    ")\n",
    "\n",
    "# Optional: compute a human-readable fail reason (first rule that tripped)\n",
    "def first_reason(i):\n",
    "    if dup_mask.iat[i]:          return \"exact_duplicate\"\n",
    "    if enc_mask.iat[i]:          return \"encoded_data\"\n",
    "    if longline_mask.iat[i]:     return \"long_lines\"\n",
    "    if binary_mask.iat[i]:       return \"binary_like\"\n",
    "    if nonascii_mask.iat[i]:     return \"too_much_nonascii\"\n",
    "    if bytes_mask.iat[i]:        return \"too_large_bytes\"\n",
    "    if lang_small_mask.iat[i]:   return \"too_few_lang_tokens\"\n",
    "    if lang_large_mask.iat[i]:   return \"too_many_lang_tokens\"\n",
    "    if no_shingles_mask.iat[i]:  return \"no_shingles\"\n",
    "    if hexnum_mask.iat[i]:       return \"hexnum_blob\"\n",
    "    if model_small_mask.iat[i]:  return \"too_few_model_tokens\"\n",
    "    return \"ok\"\n",
    "\n",
    "df = df.copy()\n",
    "df[\"quality_ok\"] = ~drop_mask\n",
    "df[\"fail_reason\"] = [first_reason(i) for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a954212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_cols = [\n",
    "    \"filename\", \"sha1\",\n",
    "    # size/lines\n",
    "    \"bytes\", \"lines\", \"avg_line_len\", \"max_line_len\",\n",
    "    # content/encoding\n",
    "    \"non_ascii_ratio\", \"binary_like\",\n",
    "    \"enc_total_matched\", \"enc_max_run\", \"enc_fraction\",\n",
    "    \"enc_hits_base64\", \"enc_hits_hexbytes\", \"enc_hits_unicode\",\n",
    "    # tokens/shingles\n",
    "    \"num_tokens_lang\", \"k_shingle\", \"num_shingles\", \"hexnum_ratio\",\n",
    "    # model tokens (optional)\n",
    "    \"num_tokens_model\",\n",
    "    # path heuristic & status\n",
    "    \"quality_ok\", \"fail_reason\",\n",
    "]\n",
    "\n",
    "candidate_df = df.loc[df[\"quality_ok\"], dedup_cols].reset_index(drop=True)\n",
    "put_back_path = Path(\"data/dropped/files_to_put_back.csv\")\n",
    "\n",
    "if put_back_path.exists():\n",
    "    put_back_set = pd.read_csv(put_back_path)\n",
    "    put_back_filenames = set(put_back_set[\"filename\"].dropna().tolist())\n",
    "else:\n",
    "    put_back_filenames = set()\n",
    "\n",
    "for fname in put_back_filenames:\n",
    "    if fname in df['filename'].values:\n",
    "        candidate_df = pd.concat([candidate_df, df[df['filename'] == fname][dedup_cols]], ignore_index=True)\n",
    "        candidate_df.loc[candidate_df['filename'] == fname, 'quality_ok'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0156792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[summary] total: 3146\n",
      "[summary] kept : 1465\n",
      "[summary] dropped: 1681\n",
      "[summary] drop reasons:\n",
      "fail_reason\n",
      "exact_duplicate         1010\n",
      "too_few_lang_tokens      557\n",
      "encoded_data              74\n",
      "hexnum_blob               19\n",
      "too_many_lang_tokens       9\n",
      "long_lines                 7\n",
      "too_large_bytes            4\n",
      "too_much_nonascii          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[summary] total:\", len(df))\n",
    "print(\"[summary] kept :\", int(df[\"quality_ok\"].sum()))\n",
    "print(\"[summary] dropped:\", int((~df[\"quality_ok\"]).sum()))\n",
    "print(\"[summary] drop reasons:\")\n",
    "print(df.loc[~df[\"quality_ok\"], \"fail_reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12ceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = df[df[\"quality_ok\"] == False].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9846b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_set = df[~df['fail_reason'].isin(['ok', 'exact_duplicate'])].copy().reset_index(drop=True)\n",
    "out_path = Path(\"data/dropped/review_files.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "review_data_set.to_csv(\"data/dropped/review_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec151e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] ==== Starting MinHash/LSH over DataFrame ====\n",
      "[info] params: K_SHINGLE=5, NUM_PERM=512, LSH_THRESHOLD=0.7\n",
      "[info] loaded 1523 files from candidate_df\n",
      "[info] files indexed   : 1523\n",
      "[diag] total candidate pairs: 264\n",
      "[diag] pairs with jaccard >= 0.7: 244\n",
      "[info] wrote CSV and Parquet to minhash_outputs/\n",
      "\n",
      "[info] ==== MinHash/LSH run summary ====\n",
      "[info] files loaded  : 1523\n",
      "[info] files indexed : 1523\n",
      "[info] files with 0 shingles (tokens < 5): 0\n",
      "[info] candidate pairs (from LSH) : 264\n",
      "[info] pairs with Jaccard >= 0.60: 258\n",
      "[info] pairs with Jaccard >= 0.70: 244\n",
      "[info] pairs with Jaccard >= 0.80: 193\n",
      "[info] pairs with Jaccard >= 0.85: 164\n",
      "[info] pairs with Jaccard >= 0.90: 135\n",
      "[info] avg Jaccard (candidates)  : 0.8771\n",
      "[info] max Jaccard               : 1.0000\n",
      "[info] min Jaccard               : 0.5200\n",
      "\n",
      "[info] top pairs:\n",
      "                                                                                   a                                                                                                    b  jaccard  a_shingles  b_shingles  union_shingles  intersect_shingles\n",
      "                                                         cryptol/examples/Cipher.cry                                                          saw-script/deps/cryptol/examples/Cipher.cry      1.0          24          24              24                  24\n",
      "                               cryptol/examples/param_modules/Common/AES_GCM_SIV.cry                                saw-script/deps/cryptol/examples/param_modules/Common/AES_GCM_SIV.cry      1.0         658         658             658                 658\n",
      "        cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES128.cry         saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES128.cry      1.0          21          21              21                  21\n",
      "                                                         cryptol/examples/Cipher.cry                                       saw-script/doc/llvm-java-verification-with-saw/code/Cipher.cry      1.0          24          24              24                  24\n",
      "                          cryptol-specs/Common/EC/PrimeField/Instantiations/P192.cry                           saw-script/deps/cryptol-specs/Common/EC/PrimeField/Instantiations/P192.cry      1.0          33          33              33                  33\n",
      "                        cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA256.cry                         saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA256.cry      1.0         246         246             246                 246\n",
      "               aws-lc-verification/cryptol-specs/Primitive/Asymmetric/Cipher/RSA.cry                                                    cryptol-specs/Primitive/Asymmetric/Cipher/RSA.cry      1.0         607         607             607                 607\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_512.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_512.cry      1.0          19          19              19                  19\n",
      "                                            cryptol/tests/modsys/functors/T012_M.cry                                             saw-script/deps/cryptol/tests/modsys/functors/T012_M.cry      1.0           5           5               5                   5\n",
      "                          cryptol-specs/Common/EC/PrimeField/Instantiations/P224.cry                           saw-script/deps/cryptol-specs/Common/EC/PrimeField/Instantiations/P224.cry      1.0          33          33              33                  33\n",
      "                                  cryptol/examples/param_modules/AES/SubByteSBox.cry                                   saw-script/deps/cryptol/examples/param_modules/AES/SubByteSBox.cry      1.0          31          31              31                  31\n",
      "                aws-lc-verification/cryptol-specs/Primitive/Keyless/Hash/Blake2b.cry                                                     cryptol-specs/Primitive/Keyless/Hash/Blake2b.cry      1.0        1230        1230            1230                1230\n",
      "                        cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA384.cry                         saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA384.cry      1.0         278         278             278                 278\n",
      "cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_128.cry saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_128.cry      1.0          29          29              29                  29\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_256.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_256.cry      1.0          19          19              19                  19\n",
      "                    aws-lc-verification/cryptol-specs/Primitive/Keyless/Hash/FNV.cry                                                         cryptol-specs/Primitive/Keyless/Hash/FNV.cry      1.0         213         213             213                 213\n",
      "                          cryptol-specs/Common/EC/PrimeField/Instantiations/P256.cry                           saw-script/deps/cryptol-specs/Common/EC/PrimeField/Instantiations/P256.cry      1.0          33          33              33                  33\n",
      "        cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES192.cry         saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES192.cry      1.0          21          21              21                  21\n",
      " cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck64_128.cry  saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck64_128.cry      1.0          29          29              29                  29\n",
      "cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_256.cry saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_256.cry      1.0          29          29              29                  29\n",
      "[info] saved hash signatures JSONL: minhash_outputs/minhash_signatures.jsonl\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.similiar_process import run_from_dataframe\n",
    "\n",
    "# candidate_df must have an absolute-path 'filename' column.\n",
    "df_files, df_pairs, similar_files = run_from_dataframe(\n",
    "    candidate_df,\n",
    "    filename_col=\"filename\",\n",
    "    root_dir=os.getenv(\"REPO_ROOT\"),  # prepended to filename when opening\n",
    "    out_dir=\"minhash_outputs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55aa4783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_shingles</th>\n",
       "      <th>num_perm</th>\n",
       "      <th>k_shingle</th>\n",
       "      <th>minhash_hashvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/cryptol/AES128.cry</td>\n",
       "      <td>1864</td>\n",
       "      <td>1729</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[2846733, 698077, 241051, 90077, 21030, 947298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/cryptol/AES128_GCM...</td>\n",
       "      <td>1118</td>\n",
       "      <td>931</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[3549557, 871518, 4108259, 3107338, 924959, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/cryptol/Asm128.cry</td>\n",
       "      <td>1188</td>\n",
       "      <td>702</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[5541488, 5402889, 241051, 1549316, 235239, 67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/cryptol/X86.cry</td>\n",
       "      <td>359</td>\n",
       "      <td>259</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[1884889, 40571396, 3229112, 373781, 775270, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AES-GCM-SIV-proof/proof/asm/deps/saw-script/do...</td>\n",
       "      <td>118</td>\n",
       "      <td>110</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>[45622433, 21472810, 10967003, 59227057, 30929...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  num_tokens  \\\n",
       "0     AES-GCM-SIV-proof/proof/asm/cryptol/AES128.cry        1864   \n",
       "1  AES-GCM-SIV-proof/proof/asm/cryptol/AES128_GCM...        1118   \n",
       "2     AES-GCM-SIV-proof/proof/asm/cryptol/Asm128.cry        1188   \n",
       "3        AES-GCM-SIV-proof/proof/asm/cryptol/X86.cry         359   \n",
       "4  AES-GCM-SIV-proof/proof/asm/deps/saw-script/do...         118   \n",
       "\n",
       "   num_shingles  num_perm  k_shingle  \\\n",
       "0          1729       512          5   \n",
       "1           931       512          5   \n",
       "2           702       512          5   \n",
       "3           259       512          5   \n",
       "4           110       512          5   \n",
       "\n",
       "                                  minhash_hashvalues  \n",
       "0  [2846733, 698077, 241051, 90077, 21030, 947298...  \n",
       "1  [3549557, 871518, 4108259, 3107338, 924959, 27...  \n",
       "2  [5541488, 5402889, 241051, 1549316, 235239, 67...  \n",
       "3  [1884889, 40571396, 3229112, 373781, 775270, 3...  \n",
       "4  [45622433, 21472810, 10967003, 59227057, 30929...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefc8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] clusters formed   : 1330\n",
      "[info] kept files        : 1330\n",
      "[info] dropped files     : 193\n",
      "[info] wrote keep/drop/cluster CSVs to minhash_outputs/\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.cluster_process import run_clustering\n",
    "\n",
    "# If you already have df_files/df_pairs in memory:\n",
    "df_keep, df_drop, df_clusters = run_clustering(\n",
    "    df_files=df_files,          # from similiar_process\n",
    "    df_pairs=df_pairs,          # from similiar_process\n",
    "    jaccard_keep_threshold=0.70,\n",
    "    out_dir=\"minhash_outputs\",\n",
    "    content_lookup=None,        # or {filename: raw_text} if you want text-derived penalties\n",
    "    save_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94f0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[df['filename'].isin(df_keep['filename'].tolist())].copy()\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'quality_ok'\n",
    "    ] = False\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'fail_reason'\n",
    "    ] = 'similiar_file_exists'\n",
    "\n",
    "dropped = df[~df['filename'].isin(dataset['filename'].tolist())].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b19640",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.to_csv(\"data/dropped/dropped_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec52c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_files = set(dataset[\"filename\"].unique())\n",
    "\n",
    "all_files_df = pd.read_json(jsonl_path, lines=True)\n",
    "all_files_filtered_df = all_files_df[all_files_df[\"filename\"].isin(verified_files)].reset_index(drop=True)\n",
    "out_path = Path(\"data/training_datasets/verified_nomods.jsonl\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "all_files_filtered_df.to_json(\"data/training_datasets/verified_nomods.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c3b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/file_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InsureHub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
