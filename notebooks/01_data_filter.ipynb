{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec3da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "os.chdir(Path(\"~/SecurityAnalytics/DataPreprocess\").expanduser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.preprocessing.quality_process import compute_file_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4302bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3126 rows\n"
     ]
    }
   ],
   "source": [
    "# Source code location\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol\n",
    "# /Users/josh/SecurityAnalytics/development/cryptol-specs\n",
    "# /Users/josh/SecurityAnalytics/development/saw-script\n",
    "\n",
    "# --- paths ---\n",
    "jsonl_path = \"data/all_sources_raw.jsonl\"      # your input dataset\n",
    "\n",
    "# --- load dataset ---\n",
    "rows = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(rows)} rows\")\n",
    "\n",
    "# --- run quality_process.py for each row ---\n",
    "results = []\n",
    "for row in rows:\n",
    "    results.append(\n",
    "        compute_file_metrics(\n",
    "            row[\"filename\"],\n",
    "            row[\"content\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- save to CSV ---\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a03ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/j77njgmn2ts9wkln0x_x_vcc0000gn/T/ipykernel_70739/2718623724.py:50: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n"
     ]
    }
   ],
   "source": [
    "# StarCoder-like thresholds (tune if needed)\n",
    "MAX_BYTES         = 200_000\n",
    "MAX_NONASCII      = 0.20\n",
    "ENC_MAX_RUN_CHARS = 1024\n",
    "ENC_MAX_FRACTION  = 0.50\n",
    "MAX_LINES_TOTAL   = 100_000\n",
    "MAX_LINE_AVG_LEN  = 100\n",
    "MAX_LINE_MAX_LEN  = 1_000\n",
    "MIN_TOKENS_LANG   = 40      # language-token gate (Cryptol tokenizer)\n",
    "MAX_TOKENS_LANG   = 10_000  # optional upper bound\n",
    "MIN_TOKENS_MODEL  = 32      # only if youâ€™ve populated num_tokens_model\n",
    "MAX_HEXNUM_RATIO  = 0.20\n",
    "\n",
    "\n",
    "# --- exact dedup (keep first occurrence of each sha1) ---\n",
    "# mark duplicates (True means \"is duplicate\" => drop later)\n",
    "dup_mask = df.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
    "\n",
    "# --- encoded data (StarCoder) ---\n",
    "enc_mask = (df[\"enc_max_run\"] > ENC_MAX_RUN_CHARS) | (df[\"enc_fraction\"] > ENC_MAX_FRACTION)\n",
    "\n",
    "# --- long-line filters (StarCoder) ---\n",
    "longline_mask = (\n",
    "    (df[\"lines\"] > MAX_LINES_TOTAL) |\n",
    "    (df[\"avg_line_len\"] > MAX_LINE_AVG_LEN) |\n",
    "    (df[\"max_line_len\"] > MAX_LINE_MAX_LEN)\n",
    ")\n",
    "\n",
    "# --- binary-like content ---\n",
    "binary_mask = df[\"binary_like\"].fillna(False)\n",
    "\n",
    "# --- non-ascii density ---\n",
    "nonascii_mask = df[\"non_ascii_ratio\"].fillna(0) > MAX_NONASCII\n",
    "\n",
    "# --- size guardrail (bytes) ---\n",
    "bytes_mask = df[\"bytes\"].fillna(0) > MAX_BYTES\n",
    "\n",
    "# --- language-token bounds ---\n",
    "lang_small_mask = df[\"num_tokens_lang\"].fillna(0) < MIN_TOKENS_LANG\n",
    "lang_large_mask = df[\"num_tokens_lang\"].fillna(0) > MAX_TOKENS_LANG\n",
    "\n",
    "# --- shingles exist (needed for Jaccard) ---\n",
    "no_shingles_mask = df[\"num_shingles\"].fillna(0) <= 0\n",
    "\n",
    "# --- numeric/hex blob concentration ---\n",
    "hexnum_mask = df[\"hexnum_ratio\"].fillna(0) > MAX_HEXNUM_RATIO\n",
    "\n",
    "# --- model-token gate (only apply where available) ---\n",
    "if \"num_tokens_model\" in df.columns:\n",
    "    model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n",
    "else:\n",
    "    model_small_mask = pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all hard-drop reasons\n",
    "drop_mask = (\n",
    "    dup_mask |\n",
    "    enc_mask |\n",
    "    longline_mask |\n",
    "    binary_mask |\n",
    "    nonascii_mask |\n",
    "    bytes_mask |\n",
    "    lang_small_mask |\n",
    "    lang_large_mask |\n",
    "    no_shingles_mask |\n",
    "    hexnum_mask |\n",
    "    model_small_mask\n",
    ")\n",
    "\n",
    "# Optional: compute a human-readable fail reason (first rule that tripped)\n",
    "def first_reason(i):\n",
    "    if dup_mask.iat[i]:          return \"exact_duplicate\"\n",
    "    if enc_mask.iat[i]:          return \"encoded_data\"\n",
    "    if longline_mask.iat[i]:     return \"long_lines\"\n",
    "    if binary_mask.iat[i]:       return \"binary_like\"\n",
    "    if nonascii_mask.iat[i]:     return \"too_much_nonascii\"\n",
    "    if bytes_mask.iat[i]:        return \"too_large_bytes\"\n",
    "    if lang_small_mask.iat[i]:   return \"too_few_lang_tokens\"\n",
    "    if lang_large_mask.iat[i]:   return \"too_many_lang_tokens\"\n",
    "    if no_shingles_mask.iat[i]:  return \"no_shingles\"\n",
    "    if hexnum_mask.iat[i]:       return \"hexnum_blob\"\n",
    "    if model_small_mask.iat[i]:  return \"too_few_model_tokens\"\n",
    "    return \"ok\"\n",
    "\n",
    "df = df.copy()\n",
    "df[\"quality_ok\"] = ~drop_mask\n",
    "df[\"fail_reason\"] = [first_reason(i) for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a954212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_cols = [\n",
    "    \"filename\", \"sha1\",\n",
    "    # size/lines\n",
    "    \"bytes\", \"lines\", \"avg_line_len\", \"max_line_len\",\n",
    "    # content/encoding\n",
    "    \"non_ascii_ratio\", \"binary_like\",\n",
    "    \"enc_total_matched\", \"enc_max_run\", \"enc_fraction\",\n",
    "    \"enc_hits_base64\", \"enc_hits_hexbytes\", \"enc_hits_unicode\",\n",
    "    # tokens/shingles\n",
    "    \"num_tokens_lang\", \"k_shingle\", \"num_shingles\", \"hexnum_ratio\",\n",
    "    # model tokens (optional)\n",
    "    \"num_tokens_model\",\n",
    "    # path heuristic & status\n",
    "    \"quality_ok\", \"fail_reason\",\n",
    "]\n",
    "\n",
    "candidate_df = df.loc[df[\"quality_ok\"], dedup_cols].reset_index(drop=True)\n",
    "put_back_set = pd.read_csv(\"data/dropped/files_to_put_back.csv\")\n",
    "put_back_filenames = set(put_back_set['filename'].tolist())\n",
    "for fname in put_back_filenames:\n",
    "    if fname in df['filename'].values:\n",
    "        candidate_df = pd.concat([candidate_df, df[df['filename'] == fname][dedup_cols]], ignore_index=True)\n",
    "        candidate_df.loc[candidate_df['filename'] == fname, 'quality_ok'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0156792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[summary] total: 3126\n",
      "[summary] kept : 1447\n",
      "[summary] dropped: 1679\n",
      "[summary] drop reasons:\n",
      "fail_reason\n",
      "exact_duplicate         1010\n",
      "too_few_lang_tokens      555\n",
      "encoded_data              74\n",
      "hexnum_blob               19\n",
      "too_many_lang_tokens       9\n",
      "long_lines                 7\n",
      "too_large_bytes            4\n",
      "too_much_nonascii          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[summary] total:\", len(df))\n",
    "print(\"[summary] kept :\", int(df[\"quality_ok\"].sum()))\n",
    "print(\"[summary] dropped:\", int((~df[\"quality_ok\"]).sum()))\n",
    "print(\"[summary] drop reasons:\")\n",
    "print(df.loc[~df[\"quality_ok\"], \"fail_reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12ceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = df[df[\"quality_ok\"] == False].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9846b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_set = df[~df['fail_reason'].isin(['ok', 'exact_duplicate'])].copy().reset_index(drop=True)\n",
    "review_data_set.to_csv(\"data/dropped/review_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16145249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec151e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] ==== Starting MinHash/LSH over DataFrame ====\n",
      "[info] params: K_SHINGLE=5, NUM_PERM=512, LSH_THRESHOLD=0.7\n",
      "[info] loaded 1504 files from candidate_df\n",
      "[info] files indexed   : 1504\n",
      "[diag] total candidate pairs: 262\n",
      "[diag] pairs with jaccard >= 0.7: 242\n",
      "[info] wrote CSV and Parquet to minhash_outputs/\n",
      "\n",
      "[info] ==== MinHash/LSH run summary ====\n",
      "[info] files loaded  : 1504\n",
      "[info] files indexed : 1504\n",
      "[info] files with 0 shingles (tokens < 5): 0\n",
      "[info] candidate pairs (from LSH) : 262\n",
      "[info] pairs with Jaccard >= 0.60: 256\n",
      "[info] pairs with Jaccard >= 0.70: 242\n",
      "[info] pairs with Jaccard >= 0.80: 191\n",
      "[info] pairs with Jaccard >= 0.85: 162\n",
      "[info] pairs with Jaccard >= 0.90: 133\n",
      "[info] avg Jaccard (candidates)  : 0.8763\n",
      "[info] max Jaccard               : 1.0000\n",
      "[info] min Jaccard               : 0.5200\n",
      "\n",
      "[info] top pairs:\n",
      "                                                                                   a                                                                                                    b  jaccard  a_shingles  b_shingles  union_shingles  intersect_shingles\n",
      "                    aws-lc-verification/cryptol-specs/Primitive/Keyless/Hash/FNV.cry                                                         cryptol-specs/Primitive/Keyless/Hash/FNV.cry      1.0         213         213             213                 213\n",
      "        cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES128.cry         saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES128.cry      1.0          21          21              21                  21\n",
      "                   cryptol/cryptol-remote-api/python/tests/cryptol/test-files/Id.cry                    saw-script/deps/cryptol/cryptol-remote-api/python/tests/cryptol/test-files/Id.cry      1.0          21          21              21                  21\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_384.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_384.cry      1.0          19          19              19                  19\n",
      "        cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES192.cry         saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/AES/Instantiations/AES192.cry      1.0          21          21              21                  21\n",
      " cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck64_128.cry  saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck64_128.cry      1.0          29          29              29                  29\n",
      "    aws-lc-verification/cryptol-specs/Primitive/Symmetric/Cipher/Block/Threefish.cry                                         cryptol-specs/Primitive/Symmetric/Cipher/Block/Threefish.cry      1.0        1467        1467            1467                1467\n",
      "                                  cryptol/examples/param_modules/AES/SubByteSBox.cry                                   saw-script/deps/cryptol/examples/param_modules/AES/SubByteSBox.cry      1.0          31          31              31                  31\n",
      "                          cryptol-specs/Common/EC/PrimeField/Instantiations/P256.cry                           saw-script/deps/cryptol-specs/Common/EC/PrimeField/Instantiations/P256.cry      1.0          33          33              33                  33\n",
      "cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_128.cry saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_128.cry      1.0          29          29              29                  29\n",
      "  cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck96_96.cry   saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck96_96.cry      1.0          29          29              29                  29\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_512.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_512.cry      1.0          19          19              19                  19\n",
      "cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_192.cry saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck128_192.cry      1.0          29          29              29                  29\n",
      "  cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck48_96.cry   saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck48_96.cry      1.0          29          29              29                  29\n",
      "  cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck48_72.cry   saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck48_72.cry      1.0          29          29              29                  29\n",
      "                        cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA256.cry                         saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA2Internal/SHA256.cry      1.0         246         246             246                 246\n",
      "                                            cryptol/tests/modsys/functors/T012_M.cry                                             saw-script/deps/cryptol/tests/modsys/functors/T012_M.cry      1.0           5           5               5                   5\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_224.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_224.cry      1.0          19          19              19                  19\n",
      " cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck96_144.cry  saw-script/deps/cryptol-specs/Primitive/Symmetric/Cipher/Block/Speck/Instantiations/Speck96_144.cry      1.0          29          29              29                  29\n",
      "               cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_256.cry                saw-script/deps/cryptol-specs/Primitive/Keyless/Hash/SHA3/Instantiations/SHA3_256.cry      1.0          19          19              19                  19\n",
      "[info] saved hash signatures JSONL: minhash_outputs/minhash_signatures.jsonl\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.similiar_process import run_from_dataframe\n",
    "\n",
    "# candidate_df must have an absolute-path 'filename' column.\n",
    "df_files, df_pairs, similar_files = run_from_dataframe(\n",
    "    candidate_df,\n",
    "    filename_col=\"filename\",\n",
    "    root_dir=\"/Users/josh/SecurityAnalytics\",  # prepended to filename when opening\n",
    "    out_dir=\"minhash_outputs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aefc8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] clusters formed   : 1312\n",
      "[info] kept files        : 1312\n",
      "[info] dropped files     : 192\n",
      "[info] wrote keep/drop/cluster CSVs to minhash_outputs/\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.cluster_process import run_clustering\n",
    "\n",
    "# If you already have df_files/df_pairs in memory:\n",
    "df_keep, df_drop, df_clusters = run_clustering(\n",
    "    df_files=df_files,          # from similiar_process\n",
    "    df_pairs=df_pairs,          # from similiar_process\n",
    "    jaccard_keep_threshold=0.70,\n",
    "    out_dir=\"minhash_outputs\",\n",
    "    content_lookup=None,        # or {filename: raw_text} if you want text-derived penalties\n",
    "    save_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94f0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[df['filename'].isin(df_keep['filename'].tolist())].copy()\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'quality_ok'\n",
    "    ] = False\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'fail_reason'\n",
    "    ] = 'similiar_file_exists'\n",
    "\n",
    "dropped = df[~df['filename'].isin(dataset['filename'].tolist())].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00b19640",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.to_csv(\"data/dropped/dropped_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec52c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_files = set(dataset[\"filename\"].unique())\n",
    "\n",
    "all_files_df = pd.read_json(jsonl_path, lines=True)\n",
    "all_files_filtered_df = all_files_df[all_files_df[\"filename\"].isin(verified_files)].reset_index(drop=True)\n",
    "\n",
    "all_files_filtered_df.to_json(\"data/training_datasets/verified_nomods.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c3b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/file_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InsureHub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
